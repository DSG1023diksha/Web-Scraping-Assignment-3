{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0764a3fc-624c-4434-99b2-d8cc0685b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\dell\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104de22b-af4f-46de-8a3c-9a27dc5a7e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.21.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a3ac17-8332-4623-a028-1a55aca44a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad77734f-4367-4623-97d8-521d600c1141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fead3d4-4bd1-4104-a907-4dba772aefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ec975-de5b-4262-a170-8eae00bb4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "   \n",
    "    search_url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}\"\n",
    "    \n",
    "    \n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the webpage.\")\n",
    "        return\n",
    "    \n",
    "   \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "  \n",
    "    product_elements = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "   \n",
    "    products = []\n",
    "    for product_element in product_elements:\n",
    "        title_element = product_element.h2.a\n",
    "        title = title_element.text.strip()\n",
    "        url = 'https://www.amazon.in' + title_element['href']\n",
    "        try:\n",
    "            price = product_element.find('span', 'a-price-whole').text\n",
    "        except AttributeError:\n",
    "            price = \"Price not available\"\n",
    "\n",
    "        products.append({\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'url': url\n",
    "        })\n",
    "    \n",
    "    \n",
    "    for idx, product in enumerate(products, 1):\n",
    "        print(f\"{idx}. {product['title']}\\n   Price: {product['price']}\\n   URL: {product['url']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the product to search on Amazon.in: \")\n",
    "    search_amazon(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea6cfc-eb22-4454-bbdc-2ef44e0ed07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_product_details(product_element):\n",
    "    try:\n",
    "        title_element = product_element.h2.a\n",
    "        title = title_element.text.strip()\n",
    "        url = 'https://www.amazon.in' + title_element['href']\n",
    "    except AttributeError:\n",
    "        title = url = '-'\n",
    "    \n",
    "    try:\n",
    "        brand = product_element.find('span', class_='a-size-base-plus').text.strip()\n",
    "    except AttributeError:\n",
    "        brand = '-'\n",
    "    \n",
    "    try:\n",
    "        price = product_element.find('span', 'a-price-whole').text.strip()\n",
    "    except AttributeError:\n",
    "        price = '-'\n",
    "    \n",
    "    \n",
    "    return_exchange = expected_delivery = availability = '-'\n",
    "\n",
    "    return {\n",
    "        'Brand Name': brand,\n",
    "        'Name of the Product': title,\n",
    "        'Price': price,\n",
    "        'Return/Exchange': return_exchange,\n",
    "        'Expected Delivery': expected_delivery,\n",
    "        'Availability': availability,\n",
    "        'Product URL': url\n",
    "    }\n",
    "\n",
    "def search_amazon(product_name):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    products = []\n",
    "    for page in range(1, 4):  # Loop through the first 3 pages\n",
    "        search_url = f\"https://www.amazon.in/s?k={product_name.replace(' ', '+')}&page={page}\"\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        product_elements = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "        \n",
    "        for product_element in product_elements:\n",
    "            product_details = get_product_details(product_element)\n",
    "            products.append(product_details)\n",
    "        \n",
    "       \n",
    "    \n",
    "    return products\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the product to search on Amazon.in: \")\n",
    "    products = search_amazon(product_name)\n",
    "    \n",
    "   \n",
    "    df = pd.DataFrame(products)\n",
    "    \n",
    "   \n",
    "    df.to_csv(f\"{product_name}_amazon_products.csv\", index=False)\n",
    "    \n",
    "    print(f\"Details of {len(products)} products have been saved to '{product_name}_amazon_products.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794eaed-f6b7-4257-9857-0a22dafe0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def search_and_download_images(search_term, num_images, download_path):\n",
    "   \n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.get('https://images.google.com')\n",
    "\n",
    "   \n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(search_term)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "   \n",
    "    scroll_pause_time = 2\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    images_urls = set()\n",
    "    while len(images_urls) < num_images:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause_time)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            try:\n",
    "                driver.find_element(By.CSS_SELECTOR, \".mye4qd\").click()\n",
    "            except:\n",
    "                break\n",
    "        last_height = new_height\n",
    "\n",
    "       \n",
    "        images = driver.find_elements(By.CSS_SELECTOR, 'img.rg_i')\n",
    "        for img in images:\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(1)\n",
    "                actual_images = driver.find_elements(By.CSS_SELECTOR, 'img.n3VNCb')\n",
    "                for actual_img in actual_images:\n",
    "                    if actual_img.get_attribute('src') and 'http' in actual_img.get_attribute('src'):\n",
    "                        images_urls.add(actual_img.get_attribute('src'))\n",
    "                        if len(images_urls) >= num_images:\n",
    "                            break\n",
    "            except:\n",
    "                continue\n",
    "        if len(images_urls) >= num_images:\n",
    "            break\n",
    "\n",
    "  \n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    for i, url in enumerate(images_urls):\n",
    "        image_name = f\"{search_term}_{i + 1}.jpg\"\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, os.path.join(download_path, image_name))\n",
    "            print(f\"Downloaded {image_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {image_name}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_terms = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    num_images = 10\n",
    "    download_path = 'downloaded_images'\n",
    "\n",
    "    for term in search_terms:\n",
    "        print(f\"Searching and downloading images for: {term}\")\n",
    "        search_and_download_images(term, num_images, download_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703923d-1f63-4d02-a501-6a3c24cf85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_smartphone_details(driver):\n",
    "    try:\n",
    "        brand_name = driver.find_element(By.CSS_SELECTOR, 'div._4rR01T').text.split()[0]\n",
    "    except:\n",
    "        brand_name = '-'\n",
    "    \n",
    "    try:\n",
    "        smartphone_name = driver.find_element(By.CSS_SELECTOR, 'div._4rR01T').text\n",
    "    except:\n",
    "        smartphone_name = '-'\n",
    "    \n",
    "    try:\n",
    "        product_url = driver.find_element(By.CSS_SELECTOR, 'a._1fQZEK').get_attribute('href')\n",
    "    except:\n",
    "        product_url = '-'\n",
    "    \n",
    "    try:\n",
    "        price = driver.find_element(By.CSS_SELECTOR, 'div._30jeq3._1_WHN1').text\n",
    "    except:\n",
    "        price = '-'\n",
    "    \n",
    "    details = driver.find_elements(By.CSS_SELECTOR, 'li.rgWa7D')\n",
    "    color = ram = storage = primary_camera = secondary_camera = display_size = battery_capacity = '-'\n",
    "\n",
    "    for detail in details:\n",
    "        text = detail.text.lower()\n",
    "        if 'color' in text:\n",
    "            color = text.split(\":\")[-1].strip()\n",
    "        elif 'ram' in text:\n",
    "            ram = text.split(\"|\")[0].strip()\n",
    "            storage = text.split(\"|\")[1].strip()\n",
    "        elif 'primary camera' in text:\n",
    "            primary_camera = text.split(\":\")[-1].strip()\n",
    "        elif 'secondary camera' in text:\n",
    "            secondary_camera = text.split(\":\")[-1].strip()\n",
    "        elif 'display size' in text:\n",
    "            display_size = text.split(\":\")[-1].strip()\n",
    "        elif 'battery capacity' in text:\n",
    "            battery_capacity = text.split(\":\")[-1].strip()\n",
    "    \n",
    "    return {\n",
    "        'Brand Name': brand_name,\n",
    "        'Smartphone Name': smartphone_name,\n",
    "        'Colour': color,\n",
    "        'RAM': ram,\n",
    "        'Storage(ROM)': storage,\n",
    "        'Primary Camera': primary_camera,\n",
    "        'Secondary Camera': secondary_camera,\n",
    "        'Display Size': display_size,\n",
    "        'Battery Capacity': battery_capacity,\n",
    "        'Price': price,\n",
    "        'Product URL': product_url\n",
    "    }\n",
    "\n",
    "def search_flipkart(smartphone_name):\n",
    "   \n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.flipkart.com')\n",
    "\n",
    "    \n",
    "    try:\n",
    "        close_button = driver.find_element(By.CSS_SELECTOR, 'button._2KpZ6l._2doB4z')\n",
    "        close_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(smartphone_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "   \n",
    "    time.sleep(3)\n",
    "\n",
    " \n",
    "    product_elements = driver.find_elements(By.CSS_SELECTOR, 'div._1AtVbE')\n",
    "\n",
    "    \n",
    "    products = []\n",
    "    for product_element in product_elements:\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", product_element)\n",
    "            product_details = get_smartphone_details(product_element)\n",
    "            products.append(product_details)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    driver.quit()\n",
    "    return products\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    smartphone_name = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "    products = search_flipkart(smartphone_name)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(products)\n",
    "    \n",
    "    \n",
    "    df.to_csv(f\"{smartphone_name}_flipkart_products.csv\", index=False)\n",
    "    \n",
    "    print(f\"Details of smartphones have been saved to '{smartphone_name}_flipkart_products.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2436a0-bb81-4db6-b01f-6f8cf9c76845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def get_geospatial_coordinates(city_name):\n",
    "    \n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.google.com/maps')\n",
    "\n",
    "    \n",
    "    search_box = driver.find_element(By.ID, 'searchboxinput')\n",
    "    search_box.send_keys(city_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "   \n",
    "    time.sleep(5)\n",
    "\n",
    "  \n",
    "    current_url = driver.current_url\n",
    "\n",
    "   \n",
    "    try:\n",
    "       \n",
    "        if \"@\" in current_url:\n",
    "            coords_part = current_url.split('@')[1].split(',')[0:2]\n",
    "            latitude, longitude = coords_part[0], coords_part[1]\n",
    "        else:\n",
    "            raise ValueError(\"Coordinates not found in the URL\")\n",
    "\n",
    "        print(f\"City: {city_name}, Latitude: {latitude}, Longitude: {longitude}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find coordinates for {city_name}. Error: {e}\")\n",
    "\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the city to search on Google Maps: \")\n",
    "    get_geospatial_coordinates(city_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11833005-7f50-4c56-8303-0f745e03d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_laptop_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    laptops = []\n",
    "\n",
    "   \n",
    "    laptop_elements = soup.find_all('div', class_='product-listing')\n",
    "    \n",
    "    for laptop in laptop_elements:\n",
    "        try:\n",
    "            title = laptop.find('h3', class_='heading').text.strip()\n",
    "        except:\n",
    "            title = '-'\n",
    "        \n",
    "        try:\n",
    "            specs = laptop.find('div', class_='Specs-Wrap').text.strip()\n",
    "        except:\n",
    "            specs = '-'\n",
    "        \n",
    "        try:\n",
    "            price = laptop.find('div', class_='price').text.strip()\n",
    "        except:\n",
    "            price = '-'\n",
    "\n",
    "        laptops.append({\n",
    "            'Title': title,\n",
    "            'Specifications': specs,\n",
    "            'Price': price\n",
    "        })\n",
    "\n",
    "    return laptops\n",
    "\n",
    "def scrape_best_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    laptops = get_laptop_details(url)\n",
    "\n",
    "   \n",
    "    df = pd.DataFrame(laptops)\n",
    "\n",
    "   \n",
    "    df.to_csv('best_gaming_laptops_digit.csv', index=False)\n",
    "    \n",
    "    print(f\"Details of the best gaming laptops have been saved to 'best_gaming_laptops_digit.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_best_gaming_laptops()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e4bd8-e53d-4006-834c-5c3ed8373bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_billionaire_details(driver):\n",
    "   \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    table = soup.find('div', {'class': 'table'})\n",
    "\n",
    "    billionaires = []\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all('div', {'class': 'table-row'})\n",
    "        for row in rows:\n",
    "            try:\n",
    "                rank = row.find('div', {'class': 'rank'}).text.strip()\n",
    "            except:\n",
    "                rank = '-'\n",
    "            try:\n",
    "                name = row.find('div', {'class': 'personName'}).text.strip()\n",
    "            except:\n",
    "                name = '-'\n",
    "            try:\n",
    "                net_worth = row.find('div', {'class': 'netWorth'}).text.strip()\n",
    "            except:\n",
    "                net_worth = '-'\n",
    "            try:\n",
    "                age = row.find('div', {'class': 'age'}).text.strip()\n",
    "            except:\n",
    "                age = '-'\n",
    "            try:\n",
    "                citizenship = row.find('div', {'class': 'countryOfCitizenship'}).text.strip()\n",
    "            except:\n",
    "                citizenship = '-'\n",
    "            try:\n",
    "                source = row.find('div', {'class': 'source'}).text.strip()\n",
    "            except:\n",
    "                source = '-'\n",
    "            try:\n",
    "                industry = row.find('div', {'class': 'category'}).text.strip()\n",
    "            except:\n",
    "                industry = '-'\n",
    "\n",
    "            billionaires.append({\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            })\n",
    "\n",
    "    return billionaires\n",
    "\n",
    "def scrape_billionaires():\n",
    "    \n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.forbes.com/billionaires/')\n",
    "\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "    \n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "\n",
    "    billionaires = get_billionaire_details(driver)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(billionaires)\n",
    "\n",
    "\n",
    "    df.to_csv('forbes_billionaires.csv', index=False)\n",
    "\n",
    "    print(f\"Details of billionaires have been saved to 'forbes_billionaires.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_billionaires()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8df381-5419-427f-a29f-46d251e0678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def initialize_youtube_client(api_key):\n",
    "    youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n",
    "    return youtube\n",
    "\n",
    "\n",
    "def fetch_video_comments(youtube, video_id, max_results=500):\n",
    "    comments = []\n",
    "\n",
    " \n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        maxResults=min(max_results, 100)\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "\n",
    "    for item in response['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']\n",
    "        comments.append({\n",
    "            'Comment': comment['textDisplay'],\n",
    "            'Comment Upvotes': comment['likeCount'],\n",
    "            'Comment Time': comment['publishedAt']\n",
    "        })\n",
    "\n",
    "    \n",
    "    while 'nextPageToken' in response and len(comments) < max_results:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=min(max_results - len(comments), 100),\n",
    "            pageToken=response['nextPageToken']\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "      \n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Comment Upvotes': comment['likeCount'],\n",
    "                'Comment Time': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "def fetch_youtube_video_comments(api_key, video_id):\n",
    "    youtube = initialize_youtube_client(api_key)\n",
    "    comments = fetch_video_comments(youtube, video_id)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(comments)\n",
    "    \n",
    "   \n",
    "    df['Comment Time'] = pd.to_datetime(df['Comment Time'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    api_key = 'YOUR_API_KEY'\n",
    "    \n",
    "  \n",
    "    video_id = 'VIDEO_ID'\n",
    "    \n",
    "   \n",
    "    comments_df = fetch_youtube_video_comments(api_key, video_id)\n",
    "    \n",
    "  \n",
    "    comments_df.to_csv('youtube_video_comments.csv', index=False)\n",
    "    \n",
    "    print(f\"Comments details have been saved to 'youtube_video_comments.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8ff07-4f9a-4c50-85ca-1577d37dbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = 'https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2024-06-23&to=2024-06-24&guests=2&page=1'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to load page: {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    hostels = []\n",
    "\n",
    "    hostel_cards = soup.find_all('div', class_='property-card')\n",
    "\n",
    "    for card in hostel_cards:\n",
    "        try:\n",
    "            name = card.find('h2', class_='title').text.strip()\n",
    "        except:\n",
    "            name = '-'\n",
    "\n",
    "        try:\n",
    "            distance = card.find('span', class_='description').text.strip()\n",
    "        except:\n",
    "            distance = '-'\n",
    "\n",
    "        try:\n",
    "            rating = card.find('div', class_='score orange big').text.strip()\n",
    "        except:\n",
    "            rating = '-'\n",
    "\n",
    "        try:\n",
    "            total_reviews = card.find('div', class_='reviews').text.strip().split()[0]\n",
    "        except:\n",
    "            total_reviews = '-'\n",
    "\n",
    "        try:\n",
    "            overall_reviews = card.find('div', class_='keyword').text.strip()\n",
    "        except:\n",
    "            overall_reviews = '-'\n",
    "\n",
    "        try:\n",
    "            private_price = card.find('a', class_='dorm-link').text.strip()\n",
    "        except:\n",
    "            private_price = '-'\n",
    "\n",
    "        try:\n",
    "            dorm_price = card.find('a', class_='dorm-link').text.strip()\n",
    "        except:\n",
    "            dorm_price = '-'\n",
    "\n",
    "        try:\n",
    "            facilities = ', '.join([item.text.strip() for item in card.find_all('i', class_='icon facility')])\n",
    "        except:\n",
    "            facilities = '-'\n",
    "\n",
    "        try:\n",
    "            description = card.find('div', class_='property-description').text.strip()\n",
    "        except:\n",
    "            description = '-'\n",
    "\n",
    "        hostels.append({\n",
    "            'Name': name,\n",
    "            'Distance from City Centre': distance,\n",
    "            'Rating': rating,\n",
    "            'Total Reviews': total_reviews,\n",
    "            'Overall Reviews': overall_reviews,\n",
    "            'Privates From Price': private_price,\n",
    "            'Dorms From Price': dorm_price,\n",
    "            'Facilities': facilities,\n",
    "            'Description': description\n",
    "        })\n",
    "\n",
    "    return hostels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hostels = scrape_hostels_in_london()\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(hostels)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv('hostels_in_london.csv', index=False)\n",
    "\n",
    "    print(f\"Hostel details have been saved to 'hostels_in_london.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769b2be-efdf-41ad-8e63-e83d1562c0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
